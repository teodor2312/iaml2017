{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introductory applied machine learning (INFR10069)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3: Object recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marking Breakdown\n",
    "\n",
    "**70-100%** results/answer correct plus extra achievement at understanding or analysis of results. Clear explanations, evidence of creative or deeper thought will contribute to a higher grade.\n",
    "\n",
    "**60-69%** results/answer correct or nearly correct and well explained.\n",
    "\n",
    "**50-59%** results/answer in right direction but significant errors.\n",
    "\n",
    "**40-49%** some evidence that the student has gained some understanding, but not answered the questions\n",
    "properly.\n",
    "\n",
    "**0-39%** serious error or slack work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mechanics\n",
    "\n",
    "Fill out this notebook, save it, and submit it **and a `.txt` file** (see Part 2) in answer to this assignment.\n",
    "**You need to submit these two files electronically as described below.**\n",
    "\n",
    "On a DICE environment, open the terminal, navigate to the location of this notebook, and submit this notebook file using the following command:\n",
    "\n",
    "`submit iaml cw2 07_Assignment_3.ipynb assignment_3_predictions.txt`\n",
    "\n",
    "What actually happens in the background is that your file is placed in a folder available to markers. If you submit a file with the same name into the same location, **it will *overwrite* your previous submission**. You can check the status of your submissions with the `show_submissions` command.\n",
    "\n",
    "**Distance Learners:** To copy your work up to DICE (such that you can use the `submit` command) you can use `scp` or `rsync` (you may need to install these yourself). You can copy files up using `student.ssh.inf.ed.ac.uk`, then ssh in to submit, e.g. (in a unix terminal):\n",
    "```\n",
    "filename1=07_Assignment_3.ipynb\n",
    "local_scp_filepath1=~/git/iaml2017/${filename1}\n",
    "filename2=assignment_3_predictions.txt\n",
    "local_scp_filepath2=~/git/iaml2017/${filename2}\n",
    "UUN=s0816700\n",
    "server_address=student.ssh.inf.ed.ac.uk\n",
    "scp -r ${local_scp_filepath1} ${UUN}@${server_address}:${filename1}\n",
    "scp -r ${local_scp_filepath2} ${UUN}@${server_address}:${filename2}\n",
    "# rsync -rl ${local_scp_filepath1} ${UUN}@${server_address}:${filename1}\n",
    "# rsync -rl ${local_scp_filepath2} ${UUN}@${server_address}:${filename2}\n",
    "ssh ${UUN}@${server_address}\n",
    "ssh student.login\n",
    "submit iaml cw2 07_Assignment_3.ipynb assignment_3_predictions.txt\n",
    "```\n",
    "\n",
    "**Late submissions:** The policy stated in the School of Informatics MSc Degree Guide is that normally you will not be allowed to submit coursework late. See http://www.inf.ed.ac.uk/teaching/years/msc/courseguide10.html#exam for exceptions to this, e.g. in case of serious medical illness or serious personal problems.\n",
    "\n",
    "**Collaboration:** You may discuss the assignment with your colleagues, provided that the writing that you submit is entirely your own. That is, you should NOT borrow actual text or code from other students. We ask that you provide a list of the people who you've had discussions with (if any).\n",
    "\n",
    "**Resubmission:** If you submit your file again, the previous submission is **overwritten**. We will mark the version that is in the submission folder at the deadline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Instructions\n",
    "\n",
    "1. You *MUST* have your environment set up as in the [README](https://github.com/JamesOwers/iaml2017) and you *must activate this environment before running this notebook*:\n",
    "```\n",
    "source activate iaml\n",
    "cd iaml_2017\n",
    "jupyter notebook\n",
    "# Navigate to this file\n",
    "```\n",
    "\n",
    "1. Wherever you are required to produce code you should use code cells, otherwise you should use markdown cells to report results and explain answers.\n",
    "\n",
    "1. The .csv files that you will be using are located at `./datasets` (the `datasets` directory is adjacent to this file).\n",
    "\n",
    "1. **IMPORTANT:** Keep your answers brief and concise. Most written questions can be answered with 2-3 lines of explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Include all required imports and execute the cell below. It's typical to include package imports at the top of the file for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of the dataset\n",
    "In this assignment our goal is to recognize objects in images of realistic scenes. There are 19 different classes of object e.g. person, dog, cat, car, etc. The dataset derives from several thousands photographs harvested from the web. Each object of a relevant class has been manually annotated with a bounding box. Images can contain none, one or multiple objects of each class. We have prepared a [website](http://www.inf.ed.ac.uk/teaching/courses/iaml/2014/assts/asst3/images.html) where you can view the images.\n",
    "\n",
    "We are going to detect whether images contain a person or not - a binary classification problem. To save you time and to make the problem manageable with limited computational resources, we have preprocessed the dataset. We will use the [Bag of Visual Words](https://en.wikipedia.org/wiki/Bag-of-words_model_in_computer_vision) representation. Each column of the dataset (which is not an lable), refers to a 'visual word'. Each image is represented by a 500 dimensional vector that contains the normalized count for each of 500 different visual words present in the respective image (a similar representation is used for the spambase dataset, just for real words). See the Appendix at the bottom of the notebook for more information. The image data is thus a $N \\times 500$ dimensional matrix where `N` is the number of images.\n",
    "\n",
    "The full dataset has 520 attributes (dimensions). The first attribute (`imgID`) contains the image ID which allows you to associate a data point with an actual image. The next 500 attributes (`dim1`, ..., `dim500`) are a normalized count vector for each visual word. The last 19 attributes (`is_class`) are the labels - 1 means the class is present in the image. In most of the experiments (unless explicitly noted otherwise) you will only need the `is_person` attribute and the 500 dimensional feature vector. **Do not use the additional class indicator attributes as features** unless explicitly told to do so. \n",
    "\n",
    "In Part A we provide you with a training (`train_images partA.csv`) and a validation (`valid_images partA .csv`) dataset. In Part B we provide three data sets: a training set (`train_images partB.csv`), a validation set (`valid_images partB.csv`), and a test set (`test_images partB.csv`). The training and validation set contain valid labels. In the test set the labels are missing. The files are available from the GitHub repository. \n",
    "\n",
    "**Important**: *Throughout the assignment you will be given various versions of the dataset that are relevant\n",
    "to a particular question. Please be careful to use the correct version of the dataset when instructed to do so.\n",
    "If you use the wrong version of the dataset by mistake no marks will be awarded.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Exploration of the dataset [70%]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.1 --- [1 mark] ==========\n",
    "Create two Pandas DataFrame objects called `train_A` and `valid_A` by loading the datasets `train_images_partA.csv` and `valid_images_partA.csv`. Display the number of data points and attributes in each of the two datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instances of training: 2093, number of attributes of training: 520\n",
      "Number of instances of validation: 1113, number of attributes of validation: 520\n"
     ]
    }
   ],
   "source": [
    "data_path = os.path.join(os.getcwd(), 'datasets', 'train_images_partA.csv')\n",
    "train_A = pd.read_csv(data_path, delimiter = ',')\n",
    "print('Number of instances of training: {}, number of attributes of training: {}'.format(train_A.shape[0], train_A.shape[1]))\n",
    "\n",
    "data_path = os.path.join(os.getcwd(), 'datasets', 'valid_images_partA.csv')\n",
    "valid_A = pd.read_csv(data_path, delimiter = ',')\n",
    "print('Number of instances of validation: {}, number of attributes of validation: {}'.format(valid_A.shape[0], valid_A.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.2 --- [1 mark] ==========\n",
    "Display and inspect the first 10 instances in the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imgId</th>\n",
       "      <th>dim1</th>\n",
       "      <th>dim2</th>\n",
       "      <th>dim3</th>\n",
       "      <th>dim4</th>\n",
       "      <th>dim5</th>\n",
       "      <th>dim6</th>\n",
       "      <th>dim7</th>\n",
       "      <th>dim8</th>\n",
       "      <th>dim9</th>\n",
       "      <th>...</th>\n",
       "      <th>is_cow</th>\n",
       "      <th>is_diningtable</th>\n",
       "      <th>is_dog</th>\n",
       "      <th>is_horse</th>\n",
       "      <th>is_motorbike</th>\n",
       "      <th>is_person</th>\n",
       "      <th>is_pottedplant</th>\n",
       "      <th>is_sheep</th>\n",
       "      <th>is_sofa</th>\n",
       "      <th>is_tvmonitor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008_000008</td>\n",
       "      <td>0.002232</td>\n",
       "      <td>0.000558</td>\n",
       "      <td>0.002790</td>\n",
       "      <td>0.000837</td>\n",
       "      <td>0.001674</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.001395</td>\n",
       "      <td>0.002232</td>\n",
       "      <td>0.003627</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2008_000015</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.000391</td>\n",
       "      <td>0.007422</td>\n",
       "      <td>0.003516</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.005078</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.002344</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2008_000019</td>\n",
       "      <td>0.000521</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001042</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.005729</td>\n",
       "      <td>0.000521</td>\n",
       "      <td>0.002083</td>\n",
       "      <td>0.003646</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2008_000023</td>\n",
       "      <td>0.002976</td>\n",
       "      <td>0.002232</td>\n",
       "      <td>0.004464</td>\n",
       "      <td>0.000372</td>\n",
       "      <td>0.000372</td>\n",
       "      <td>0.002232</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003720</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2008_000028</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.002378</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003397</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2008_000033</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006324</td>\n",
       "      <td>0.000372</td>\n",
       "      <td>0.000372</td>\n",
       "      <td>0.000372</td>\n",
       "      <td>0.000372</td>\n",
       "      <td>0.000744</td>\n",
       "      <td>0.008185</td>\n",
       "      <td>0.000372</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2008_000036</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004416</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.006114</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.002717</td>\n",
       "      <td>0.003057</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2008_000037</td>\n",
       "      <td>0.000837</td>\n",
       "      <td>0.002232</td>\n",
       "      <td>0.000279</td>\n",
       "      <td>0.000279</td>\n",
       "      <td>0.000837</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000279</td>\n",
       "      <td>0.006696</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2008_000041</td>\n",
       "      <td>0.002378</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.004755</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.003736</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.004076</td>\n",
       "      <td>0.003397</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2008_000045</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.006454</td>\n",
       "      <td>0.001698</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.003736</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004076</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 520 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         imgId      dim1      dim2      dim3      dim4      dim5      dim6  \\\n",
       "0  2008_000008  0.002232  0.000558  0.002790  0.000837  0.001674  0.001953   \n",
       "1  2008_000015  0.001563  0.000391  0.007422  0.003516  0.003906  0.005078   \n",
       "2  2008_000019  0.000521  0.000000  0.000000  0.001042  0.001563  0.005729   \n",
       "3  2008_000023  0.002976  0.002232  0.004464  0.000372  0.000372  0.002232   \n",
       "4  2008_000028  0.001359  0.000340  0.001359  0.000340  0.001359  0.002038   \n",
       "5  2008_000033  0.000000  0.006324  0.000372  0.000372  0.000372  0.000372   \n",
       "6  2008_000036  0.000340  0.000000  0.004416  0.000340  0.000679  0.006114   \n",
       "7  2008_000037  0.000837  0.002232  0.000279  0.000279  0.000837  0.000000   \n",
       "8  2008_000041  0.002378  0.001359  0.004755  0.001019  0.003736  0.001359   \n",
       "9  2008_000045  0.001019  0.000340  0.006454  0.001698  0.001359  0.003736   \n",
       "\n",
       "       dim7      dim8      dim9      ...       is_cow  is_diningtable  is_dog  \\\n",
       "0  0.001395  0.002232  0.003627      ...            0               0       0   \n",
       "1  0.001953  0.002344  0.001953      ...            0               0       0   \n",
       "2  0.000521  0.002083  0.003646      ...            0               0       1   \n",
       "3  0.000000  0.003720  0.000000      ...            0               0       0   \n",
       "4  0.002378  0.000000  0.003397      ...            0               0       0   \n",
       "5  0.000744  0.008185  0.000372      ...            0               0       0   \n",
       "6  0.001359  0.002717  0.003057      ...            0               0       0   \n",
       "7  0.000279  0.006696  0.000000      ...            0               0       0   \n",
       "8  0.001019  0.004076  0.003397      ...            0               1       0   \n",
       "9  0.000000  0.004076  0.000000      ...            0               0       0   \n",
       "\n",
       "   is_horse  is_motorbike  is_person  is_pottedplant  is_sheep  is_sofa  \\\n",
       "0         1             0          1               0         0        0   \n",
       "1         0             0          0               0         0        0   \n",
       "2         0             0          0               0         0        0   \n",
       "3         0             0          1               0         0        0   \n",
       "4         0             0          0               0         0        0   \n",
       "5         0             0          0               0         0        0   \n",
       "6         0             0          1               0         0        0   \n",
       "7         0             0          0               0         0        0   \n",
       "8         0             0          1               0         0        0   \n",
       "9         0             0          0               0         0        0   \n",
       "\n",
       "   is_tvmonitor  \n",
       "0             0  \n",
       "1             0  \n",
       "2             0  \n",
       "3             1  \n",
       "4             0  \n",
       "5             0  \n",
       "6             0  \n",
       "7             0  \n",
       "8             0  \n",
       "9             0  \n",
       "\n",
       "[10 rows x 520 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_A.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.3 --- [1 mark] ==========\n",
    "Select the attributes (i.e. input features) for training our classifiers. These should be the visual word normalised counts `dim1, dim2, ..., dim500`. Create a list of the **names** of the attributes of interest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dim1',\n",
       " 'dim2',\n",
       " 'dim3',\n",
       " 'dim4',\n",
       " 'dim5',\n",
       " 'dim6',\n",
       " 'dim7',\n",
       " 'dim8',\n",
       " 'dim9',\n",
       " 'dim10',\n",
       " 'dim11',\n",
       " 'dim12',\n",
       " 'dim13',\n",
       " 'dim14',\n",
       " 'dim15',\n",
       " 'dim16',\n",
       " 'dim17',\n",
       " 'dim18',\n",
       " 'dim19',\n",
       " 'dim20',\n",
       " 'dim21',\n",
       " 'dim22',\n",
       " 'dim23',\n",
       " 'dim24',\n",
       " 'dim25',\n",
       " 'dim26',\n",
       " 'dim27',\n",
       " 'dim28',\n",
       " 'dim29',\n",
       " 'dim30',\n",
       " 'dim31',\n",
       " 'dim32',\n",
       " 'dim33',\n",
       " 'dim34',\n",
       " 'dim35',\n",
       " 'dim36',\n",
       " 'dim37',\n",
       " 'dim38',\n",
       " 'dim39',\n",
       " 'dim40',\n",
       " 'dim41',\n",
       " 'dim42',\n",
       " 'dim43',\n",
       " 'dim44',\n",
       " 'dim45',\n",
       " 'dim46',\n",
       " 'dim47',\n",
       " 'dim48',\n",
       " 'dim49',\n",
       " 'dim50',\n",
       " 'dim51',\n",
       " 'dim52',\n",
       " 'dim53',\n",
       " 'dim54',\n",
       " 'dim55',\n",
       " 'dim56',\n",
       " 'dim57',\n",
       " 'dim58',\n",
       " 'dim59',\n",
       " 'dim60',\n",
       " 'dim61',\n",
       " 'dim62',\n",
       " 'dim63',\n",
       " 'dim64',\n",
       " 'dim65',\n",
       " 'dim66',\n",
       " 'dim67',\n",
       " 'dim68',\n",
       " 'dim69',\n",
       " 'dim70',\n",
       " 'dim71',\n",
       " 'dim72',\n",
       " 'dim73',\n",
       " 'dim74',\n",
       " 'dim75',\n",
       " 'dim76',\n",
       " 'dim77',\n",
       " 'dim78',\n",
       " 'dim79',\n",
       " 'dim80',\n",
       " 'dim81',\n",
       " 'dim82',\n",
       " 'dim83',\n",
       " 'dim84',\n",
       " 'dim85',\n",
       " 'dim86',\n",
       " 'dim87',\n",
       " 'dim88',\n",
       " 'dim89',\n",
       " 'dim90',\n",
       " 'dim91',\n",
       " 'dim92',\n",
       " 'dim93',\n",
       " 'dim94',\n",
       " 'dim95',\n",
       " 'dim96',\n",
       " 'dim97',\n",
       " 'dim98',\n",
       " 'dim99',\n",
       " 'dim100',\n",
       " 'dim101',\n",
       " 'dim102',\n",
       " 'dim103',\n",
       " 'dim104',\n",
       " 'dim105',\n",
       " 'dim106',\n",
       " 'dim107',\n",
       " 'dim108',\n",
       " 'dim109',\n",
       " 'dim110',\n",
       " 'dim111',\n",
       " 'dim112',\n",
       " 'dim113',\n",
       " 'dim114',\n",
       " 'dim115',\n",
       " 'dim116',\n",
       " 'dim117',\n",
       " 'dim118',\n",
       " 'dim119',\n",
       " 'dim120',\n",
       " 'dim121',\n",
       " 'dim122',\n",
       " 'dim123',\n",
       " 'dim124',\n",
       " 'dim125',\n",
       " 'dim126',\n",
       " 'dim127',\n",
       " 'dim128',\n",
       " 'dim129',\n",
       " 'dim130',\n",
       " 'dim131',\n",
       " 'dim132',\n",
       " 'dim133',\n",
       " 'dim134',\n",
       " 'dim135',\n",
       " 'dim136',\n",
       " 'dim137',\n",
       " 'dim138',\n",
       " 'dim139',\n",
       " 'dim140',\n",
       " 'dim141',\n",
       " 'dim142',\n",
       " 'dim143',\n",
       " 'dim144',\n",
       " 'dim145',\n",
       " 'dim146',\n",
       " 'dim147',\n",
       " 'dim148',\n",
       " 'dim149',\n",
       " 'dim150',\n",
       " 'dim151',\n",
       " 'dim152',\n",
       " 'dim153',\n",
       " 'dim154',\n",
       " 'dim155',\n",
       " 'dim156',\n",
       " 'dim157',\n",
       " 'dim158',\n",
       " 'dim159',\n",
       " 'dim160',\n",
       " 'dim161',\n",
       " 'dim162',\n",
       " 'dim163',\n",
       " 'dim164',\n",
       " 'dim165',\n",
       " 'dim166',\n",
       " 'dim167',\n",
       " 'dim168',\n",
       " 'dim169',\n",
       " 'dim170',\n",
       " 'dim171',\n",
       " 'dim172',\n",
       " 'dim173',\n",
       " 'dim174',\n",
       " 'dim175',\n",
       " 'dim176',\n",
       " 'dim177',\n",
       " 'dim178',\n",
       " 'dim179',\n",
       " 'dim180',\n",
       " 'dim181',\n",
       " 'dim182',\n",
       " 'dim183',\n",
       " 'dim184',\n",
       " 'dim185',\n",
       " 'dim186',\n",
       " 'dim187',\n",
       " 'dim188',\n",
       " 'dim189',\n",
       " 'dim190',\n",
       " 'dim191',\n",
       " 'dim192',\n",
       " 'dim193',\n",
       " 'dim194',\n",
       " 'dim195',\n",
       " 'dim196',\n",
       " 'dim197',\n",
       " 'dim198',\n",
       " 'dim199',\n",
       " 'dim200',\n",
       " 'dim201',\n",
       " 'dim202',\n",
       " 'dim203',\n",
       " 'dim204',\n",
       " 'dim205',\n",
       " 'dim206',\n",
       " 'dim207',\n",
       " 'dim208',\n",
       " 'dim209',\n",
       " 'dim210',\n",
       " 'dim211',\n",
       " 'dim212',\n",
       " 'dim213',\n",
       " 'dim214',\n",
       " 'dim215',\n",
       " 'dim216',\n",
       " 'dim217',\n",
       " 'dim218',\n",
       " 'dim219',\n",
       " 'dim220',\n",
       " 'dim221',\n",
       " 'dim222',\n",
       " 'dim223',\n",
       " 'dim224',\n",
       " 'dim225',\n",
       " 'dim226',\n",
       " 'dim227',\n",
       " 'dim228',\n",
       " 'dim229',\n",
       " 'dim230',\n",
       " 'dim231',\n",
       " 'dim232',\n",
       " 'dim233',\n",
       " 'dim234',\n",
       " 'dim235',\n",
       " 'dim236',\n",
       " 'dim237',\n",
       " 'dim238',\n",
       " 'dim239',\n",
       " 'dim240',\n",
       " 'dim241',\n",
       " 'dim242',\n",
       " 'dim243',\n",
       " 'dim244',\n",
       " 'dim245',\n",
       " 'dim246',\n",
       " 'dim247',\n",
       " 'dim248',\n",
       " 'dim249',\n",
       " 'dim250',\n",
       " 'dim251',\n",
       " 'dim252',\n",
       " 'dim253',\n",
       " 'dim254',\n",
       " 'dim255',\n",
       " 'dim256',\n",
       " 'dim257',\n",
       " 'dim258',\n",
       " 'dim259',\n",
       " 'dim260',\n",
       " 'dim261',\n",
       " 'dim262',\n",
       " 'dim263',\n",
       " 'dim264',\n",
       " 'dim265',\n",
       " 'dim266',\n",
       " 'dim267',\n",
       " 'dim268',\n",
       " 'dim269',\n",
       " 'dim270',\n",
       " 'dim271',\n",
       " 'dim272',\n",
       " 'dim273',\n",
       " 'dim274',\n",
       " 'dim275',\n",
       " 'dim276',\n",
       " 'dim277',\n",
       " 'dim278',\n",
       " 'dim279',\n",
       " 'dim280',\n",
       " 'dim281',\n",
       " 'dim282',\n",
       " 'dim283',\n",
       " 'dim284',\n",
       " 'dim285',\n",
       " 'dim286',\n",
       " 'dim287',\n",
       " 'dim288',\n",
       " 'dim289',\n",
       " 'dim290',\n",
       " 'dim291',\n",
       " 'dim292',\n",
       " 'dim293',\n",
       " 'dim294',\n",
       " 'dim295',\n",
       " 'dim296',\n",
       " 'dim297',\n",
       " 'dim298',\n",
       " 'dim299',\n",
       " 'dim300',\n",
       " 'dim301',\n",
       " 'dim302',\n",
       " 'dim303',\n",
       " 'dim304',\n",
       " 'dim305',\n",
       " 'dim306',\n",
       " 'dim307',\n",
       " 'dim308',\n",
       " 'dim309',\n",
       " 'dim310',\n",
       " 'dim311',\n",
       " 'dim312',\n",
       " 'dim313',\n",
       " 'dim314',\n",
       " 'dim315',\n",
       " 'dim316',\n",
       " 'dim317',\n",
       " 'dim318',\n",
       " 'dim319',\n",
       " 'dim320',\n",
       " 'dim321',\n",
       " 'dim322',\n",
       " 'dim323',\n",
       " 'dim324',\n",
       " 'dim325',\n",
       " 'dim326',\n",
       " 'dim327',\n",
       " 'dim328',\n",
       " 'dim329',\n",
       " 'dim330',\n",
       " 'dim331',\n",
       " 'dim332',\n",
       " 'dim333',\n",
       " 'dim334',\n",
       " 'dim335',\n",
       " 'dim336',\n",
       " 'dim337',\n",
       " 'dim338',\n",
       " 'dim339',\n",
       " 'dim340',\n",
       " 'dim341',\n",
       " 'dim342',\n",
       " 'dim343',\n",
       " 'dim344',\n",
       " 'dim345',\n",
       " 'dim346',\n",
       " 'dim347',\n",
       " 'dim348',\n",
       " 'dim349',\n",
       " 'dim350',\n",
       " 'dim351',\n",
       " 'dim352',\n",
       " 'dim353',\n",
       " 'dim354',\n",
       " 'dim355',\n",
       " 'dim356',\n",
       " 'dim357',\n",
       " 'dim358',\n",
       " 'dim359',\n",
       " 'dim360',\n",
       " 'dim361',\n",
       " 'dim362',\n",
       " 'dim363',\n",
       " 'dim364',\n",
       " 'dim365',\n",
       " 'dim366',\n",
       " 'dim367',\n",
       " 'dim368',\n",
       " 'dim369',\n",
       " 'dim370',\n",
       " 'dim371',\n",
       " 'dim372',\n",
       " 'dim373',\n",
       " 'dim374',\n",
       " 'dim375',\n",
       " 'dim376',\n",
       " 'dim377',\n",
       " 'dim378',\n",
       " 'dim379',\n",
       " 'dim380',\n",
       " 'dim381',\n",
       " 'dim382',\n",
       " 'dim383',\n",
       " 'dim384',\n",
       " 'dim385',\n",
       " 'dim386',\n",
       " 'dim387',\n",
       " 'dim388',\n",
       " 'dim389',\n",
       " 'dim390',\n",
       " 'dim391',\n",
       " 'dim392',\n",
       " 'dim393',\n",
       " 'dim394',\n",
       " 'dim395',\n",
       " 'dim396',\n",
       " 'dim397',\n",
       " 'dim398',\n",
       " 'dim399',\n",
       " 'dim400',\n",
       " 'dim401',\n",
       " 'dim402',\n",
       " 'dim403',\n",
       " 'dim404',\n",
       " 'dim405',\n",
       " 'dim406',\n",
       " 'dim407',\n",
       " 'dim408',\n",
       " 'dim409',\n",
       " 'dim410',\n",
       " 'dim411',\n",
       " 'dim412',\n",
       " 'dim413',\n",
       " 'dim414',\n",
       " 'dim415',\n",
       " 'dim416',\n",
       " 'dim417',\n",
       " 'dim418',\n",
       " 'dim419',\n",
       " 'dim420',\n",
       " 'dim421',\n",
       " 'dim422',\n",
       " 'dim423',\n",
       " 'dim424',\n",
       " 'dim425',\n",
       " 'dim426',\n",
       " 'dim427',\n",
       " 'dim428',\n",
       " 'dim429',\n",
       " 'dim430',\n",
       " 'dim431',\n",
       " 'dim432',\n",
       " 'dim433',\n",
       " 'dim434',\n",
       " 'dim435',\n",
       " 'dim436',\n",
       " 'dim437',\n",
       " 'dim438',\n",
       " 'dim439',\n",
       " 'dim440',\n",
       " 'dim441',\n",
       " 'dim442',\n",
       " 'dim443',\n",
       " 'dim444',\n",
       " 'dim445',\n",
       " 'dim446',\n",
       " 'dim447',\n",
       " 'dim448',\n",
       " 'dim449',\n",
       " 'dim450',\n",
       " 'dim451',\n",
       " 'dim452',\n",
       " 'dim453',\n",
       " 'dim454',\n",
       " 'dim455',\n",
       " 'dim456',\n",
       " 'dim457',\n",
       " 'dim458',\n",
       " 'dim459',\n",
       " 'dim460',\n",
       " 'dim461',\n",
       " 'dim462',\n",
       " 'dim463',\n",
       " 'dim464',\n",
       " 'dim465',\n",
       " 'dim466',\n",
       " 'dim467',\n",
       " 'dim468',\n",
       " 'dim469',\n",
       " 'dim470',\n",
       " 'dim471',\n",
       " 'dim472',\n",
       " 'dim473',\n",
       " 'dim474',\n",
       " 'dim475',\n",
       " 'dim476',\n",
       " 'dim477',\n",
       " 'dim478',\n",
       " 'dim479',\n",
       " 'dim480',\n",
       " 'dim481',\n",
       " 'dim482',\n",
       " 'dim483',\n",
       " 'dim484',\n",
       " 'dim485',\n",
       " 'dim486',\n",
       " 'dim487',\n",
       " 'dim488',\n",
       " 'dim489',\n",
       " 'dim490',\n",
       " 'dim491',\n",
       " 'dim492',\n",
       " 'dim493',\n",
       " 'dim494',\n",
       " 'dim495',\n",
       " 'dim496',\n",
       " 'dim497',\n",
       " 'dim498',\n",
       " 'dim499',\n",
       " 'dim500']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = [\"dim\" + str(i) for i in range(1, 501)]\n",
    "names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.4 --- [1 mark] ==========\n",
    "By using the list from Question 1.3 now create 4 numpy arrays `X_tr`, `X_val`, `y_tr` and `y_val` in which to store the training features, validation features, training targets, and validation targets, respectively. Your target vectors should correspond to the `is_person` attribute of the training and validation sets. Display the dimensionalities (i.e shapes) of the 4 arrays. \n",
    "\n",
    "Check this carefully - you will be penalised in following questions if the data is not correct!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_tr: (2093, 500)\n",
      "y_tr: (2093,)\n",
      "X_val: (1113, 500)\n",
      "y_val: (1113,)\n"
     ]
    }
   ],
   "source": [
    "X_tr = train_A[names]\n",
    "y_tr = train_A[\"is_person\"]\n",
    "X_val = valid_A[names]\n",
    "y_val = valid_A[\"is_person\"]\n",
    "print(\"X_tr: {}\\ny_tr: {}\\nX_val: {}\\ny_val: {}\".format(X_tr.shape, y_tr.shape, X_val.shape, y_val.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.5 --- [2 marks] ==========\n",
    "Create a [countplots](https://seaborn.github.io/generated/seaborn.countplot.html?highlight=countplot#seaborn.countplot) for the training and validation targets. Create a single figure, and put the two plots inside the single figure. Label axes appropriately and add a title to your plot. Use descriptive `xticklabels` instead of the default numeric ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY8AAAEVCAYAAAAYZ2nCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGdUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a0e778110>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1,2)\n",
    "sns.countplot(y_tr, ax=ax1)\n",
    "ax1.set_title(\"Training set\")\n",
    "ax1.set_xticklabels([\"Not Person\", \"Person\"])\n",
    "ax1.set_xlabel(\"\")\n",
    "sns.countplot(y_val, ax=ax2)\n",
    "ax2.set_title(\"Validation set\")\n",
    "ax2.set_xticklabels([\"Not Person\", \"Person\"])\n",
    "ax2.set_xlabel(\"\")\n",
    "ax2.set_ylabel(\"\")\n",
    "fig.suptitle(\"Not Person vs Person for Training and Validation sets\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.6 --- [1 mark] ==========\n",
    "We want to make a baseline classification accuracy to beat. Pick a baseline \"dummy\" classifier, describe in a sentence why you chose it, and report the accuracy it achieves on the validation set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dummy classifier that always chooses the most frequent label. Very basic model as a starting point, it already predicts correctly a little above 50% of the time on tha validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification accuracy on validation set: 0.527\n"
     ]
    }
   ],
   "source": [
    "clf = DummyClassifier(strategy='most_frequent', random_state=0)\n",
    "clf.fit(X_tr, y_tr)\n",
    "print('Classification accuracy on validation set: {:.3f}'.format(clf.score(X_val, y_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.7 --- [3 marks] ==========\n",
    "Train a [`LogisticRegression`](http://scikit-learn.org/0.17/modules/generated/sklearn.linear_model.LogisticRegression.html) classifier using default settings, except for the `solver` parameter which you should set to `lbfgs`. Report the classification accuracy score on the training and validation sets and compare with the baseline. Comment on the results with 1-2 sentences. You may include any additional plot(s) if you wish to justify your explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification accuracy on validation set: 0.527\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(solver='lbfgs')\n",
    "lr.fit(X_tr, y_tr)\n",
    "print('Classification accuracy on validation set: {:.3f}'.format(lr.score(X_val, y_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Logistic Regression model is not doing a great job, it's prediction is the same as the baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.8 --- [1 mark] ==========\n",
    "Display the means and standard deviations of the first 5 features in the training set. *Hint: you want to compute the means and standard deviations for each column in your arrays. Make sure you make appropriate use of the `axis` parameter.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim1\n",
      "Mean: 0.002\n",
      "Std : 0.001\n",
      "dim2\n",
      "Mean: 0.001\n",
      "Std : 0.001\n",
      "dim3\n",
      "Mean: 0.004\n",
      "Std : 0.004\n",
      "dim4\n",
      "Mean: 0.002\n",
      "Std : 0.001\n",
      "dim5\n",
      "Mean: 0.002\n",
      "Std : 0.002\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 5):\n",
    "    print(names[i])\n",
    "    print(\"Mean: {:.3f}\\nStd : {:.3f}\".format(np.mean(X_tr[names[i]]), np.std(X_tr[names[i]])))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.9 --- [3 marks] ==========\n",
    "Feature standardisation is a pre-processing technique used to transform data so that they have zero mean and unit standard deviation. For many algorithms, this is a very important step for training models (both regression and classification). Read about [feature standardisation](http://scikit-learn.org/0.17/modules/preprocessing.html) and make sure you understand what kind of transformation this method applies to the data.\n",
    "\n",
    "`Scikit-learn` offers a [class](http://scikit-learn.org/0.17/modules/generated/sklearn.preprocessing.StandardScaler.html) for feature standardisation. Instansiate a StandardScaler object and fit it using the **training features**. Use this fitted object to transform both your training and validation features to have a standard scale. \n",
    "\n",
    "Once your training and validation input data have been transformed, display the means and standard deviations of the first 5 attributes for **both** the training and validation sets. Are the results as you expected? Explain your answer in 2-3 sentences. Why didn't we use the validation set to standardise the data?\n",
    "\n",
    "**IMPORTANT: You should use the transformed data for the rest of this part.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set\n",
      "dim1 :\n",
      "Mean: -0.000\n",
      "Std : 1.000\n",
      "dim2 :\n",
      "Mean: -0.000\n",
      "Std : 1.000\n",
      "dim3 :\n",
      "Mean: 0.000\n",
      "Std : 1.000\n",
      "dim4 :\n",
      "Mean: 0.000\n",
      "Std : 1.000\n",
      "dim5 :\n",
      "Mean: -0.000\n",
      "Std : 1.000\n",
      "\n",
      "Validation set\n",
      "dim1 :\n",
      "Mean: -0.006\n",
      "Std : 1.013\n",
      "dim2 :\n",
      "Mean: -0.038\n",
      "Std : 0.970\n",
      "dim3 :\n",
      "Mean: 0.109\n",
      "Std : 1.049\n",
      "dim4 :\n",
      "Mean: 0.094\n",
      "Std : 1.057\n",
      "dim5 :\n",
      "Mean: 0.031\n",
      "Std : 0.975\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler().fit(X_tr)\n",
    "X_tr2 = scaler.transform(X_tr)\n",
    "X_val2 = scaler.transform(X_val)\n",
    "print(\"Training set\")\n",
    "for i in range(0, 5):\n",
    "    print(names[i], \":\")\n",
    "    print(\"Mean: {:.3f}\\nStd : {:.3f}\".format(X_tr2[:,i].mean(), X_tr2[:,i].std()))\n",
    "\n",
    "print(\"\\nValidation set\")\n",
    "for i in range(0, 5):\n",
    "    print(names[i], \":\")\n",
    "    print(\"Mean: {:.3f}\\nStd : {:.3f}\".format(X_val2[:,i].mean(), X_val2[:,i].std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results are as expected, since we used the training set to standardise the data, the trainging set has means 0 and deviations 1. Validation set is similar to the training one, but not the same that's why its means are close around 0 and deviations close to 1, but never 0 or 1. Since we're training on the training set it should be the one that's standardised."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.10 --- [3 marks] ==========\n",
    "By using the transformed input data, train a new `LogisticRegression` classifier. Again, set the `solver` parameter to `lbfgs` and use default settings for the other parameters. Report the classification accuracy on both the training and validation sets.\n",
    "\n",
    "Comment on how your model compares to the baseline classifier from Question 1.6? You may use additional plot(s) to support your explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification accuracy on training set: 0.811\n",
      "Classification accuracy on validation set: 0.643\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(solver='lbfgs')\n",
    "lr.fit(X_tr2, y_tr)\n",
    "print('Classification accuracy on training set: {:.3f}'.format(lr.score(X_tr2, y_tr)))\n",
    "print('Classification accuracy on validation set: {:.3f}'.format(lr.score(X_val2, y_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a significat increse in preformance compared to the baseline, going from 0.527 to 0.642 accuracy on the validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.11 --- [1 mark] ==========\n",
    "So far we have used default settings for training the logistic regression classifier. Now, we want to use [K-fold cross-validation](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html) to optimise the regularisation parameter `C`. The regularisation parameter controls the degree to which we wish to penalise large magnitudes in the weight vector. This can help us prevent overfitting but, if set too high, could lead us to underfit too.\n",
    "\n",
    "Create a 3-fold cross-validation object. Set the `shuffle` parameter to `True` and the `random_state` to `0`. By using the cross-validation iterator, display the number of test samples for each fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Number of samples: 698\n",
      "Fold 2\n",
      "Number of samples: 698\n",
      "Fold 3\n",
      "Number of samples: 697\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=0)\n",
    "kf = kf.split(X_tr2)\n",
    "for kf_train, kf_test in kf:\n",
    "    i += 1\n",
    "    print(\"Fold {}\\nNumber of samples: {}\".format(i, len(kf_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.12 --- [2 marks] ========== \n",
    "Using the cross-validation iterator from the previous question, perform a search for the best value for `C`. \n",
    "\n",
    "We are going to loop over each CV fold, and each value of `C`. The values of `C` to search should be 20 equally-spaced values **in log space** ranging from `1e-5` to `1e5` *(hint: look at the `logspace()` function in numpy)*. \n",
    "\n",
    "Create a 2-dimensional array and, for each cross-validation fold and parameter setting pair, compute and store the classification accuracy score e.g. store the score of fold 0 with parameter setting 1 at score_array[0,1]. As previously, set the `solver` parameter to `lbfgs` and use default settings for the other parameters (except for `C` obviously!).\n",
    "\n",
    "*(hint: you could use two loops in your code; one iterating over CV folds and another one iterating over the values for `C`)*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=3, shuffle=True, random_state=0)\n",
    "kf = kf.split(X_tr2)\n",
    "i = 0\n",
    "arr = np.ndarray((3,20))\n",
    "for kf_train, kf_test in kf:\n",
    "    j = 0\n",
    "    for c in np.logspace(start=-5, stop=5, num=20):\n",
    "        lr = LogisticRegression(solver=\"lbfgs\", C=c)\n",
    "        lr.fit(X_tr2[kf_train], y_tr[kf_train]) \n",
    "        arr[i][j] = (lr.score(X_tr2[kf_test], y_tr[kf_test]))\n",
    "        j +=1\n",
    "    i +=1\n",
    "print (arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.13 --- [1 mark] ========== \n",
    "Plot the mean classification performance (across CV folds) of the logistic regression classifier against the regularisation parameter `C` by using the range from Question 1.12. Use a logarithmic scale for the x-axis and label both axes appropriately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logspace = np.logspace(start=-5, stop=5, num=20)\n",
    "mean_arr = arr.mean(axis=0)\n",
    "plt.plot(logspace, mean_arr)\n",
    "plt.semilogx()\n",
    "plt.xlabel(\"C\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Classification Preformance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.14 --- [2 marks] ==========\n",
    "Display the optimal value for the regularisation parameter `C` determined by the search results from Question 1.12. Similarly to Question 1.13, consider the mean classifiation accuracy across CV folds. By using the optimal value (i.e. the one that yields the highest average classification accuracy) train a new `LogisticRegression` classifier and report the classification accuracy on the validation set. *(Hint: Do not pick the optimal value \"by hand\", instead use an appropriate numpy function).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_c = logspace[mean_arr.argmax()]\n",
    "lr = LogisticRegression(solver=\"lbfgs\", C=optimal_c)\n",
    "lr.fit(X_tr2, y_tr)\n",
    "print(\"Optimal C: {:.3f}\".format(optimal_c))\n",
    "print('Classification accuracy on validation set: {:.3f}'.format(lr.score(X_val2, y_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.15 --- [1 mark] ========== \n",
    "Scikit-learn offers a [`LogisticRegressionCV`](http://scikit-learn.org/0.17/modules/generated/sklearn.linear_model.LogisticRegressionCV.html) module which implements Logistic Regression with builtin cross-validation to find out the optimal `C` parameter. You can specify the range for the `C` parameter, as well as the cross-validation method you want to use with the `Cs` and `cv` parameters, respectively. Use the `C` range you set up in Question 1.12 and the 3-fold cross-validation iterator from Question 1.11. Once again, train the models by using the `lbfgs` optimisation method and display the optimal value for the parameter `C`. Finally, display the classification accuracy on the validation set. Check your results are consistent with those from Question 1.14!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=3, shuffle=True, random_state=0)\n",
    "kf = kf.split(X_tr2)\n",
    "lrcv = LogisticRegressionCV(Cs=logspace, cv=kf, solver='lbfgs')\n",
    "lrcv.fit(X=X_tr2, y=y_tr)\n",
    "\n",
    "optimal_c = lrcv.C_\n",
    "\n",
    "print(\"Optimal C: {:.3f}\".format(optimal_c[0]))\n",
    "print('Classification accuracy on validation set: {:.3f}'.format(lr.score(X_val2, y_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.16 --- [1 mark] ==========\n",
    "Now, we want to validate the importance of various features for classification. For this purpose, we will use a [`RandomForestClassifier`](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) (you might want to refer to the labs if you are unsure how we can estimate feature importances with decision tree and random forest models).\n",
    "\n",
    "Initialise a random forest classifier and fit the model by using training data only and 500 trees (i.e. `n_estimators`). Set the `RandomState` equal to 42 to ensure reproducible results. Report the accuracy score on both the training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=500, random_state=42)\n",
    "rf.fit(X_tr2, y_tr)\n",
    "print('Classification accuracy on training set: {:.3f}'.format(accuracy_score(y_tr, rf.predict(X_tr2))))\n",
    "print('Classification accuracy on validation set: {:.3f}'.format(accuracy_score(y_val, rf.predict(X_val2))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.17 --- [2 marks] ==========\n",
    "Comment on the results above. Do you find the discrepancy between training and validation accuracies surprising?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not suprising at all. There is no max_depth set so the random forest classifier is overfitting on the training set, thus it get's a perfect accuracy, while the validation set preforms about 30% worse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.18 --- [2 marks] ==========\n",
    "By using the random forest model from the previous question order the features by descending importance and display the names of the 50 most important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.argsort(rf.feature_importances_)[::-1]\n",
    "num = 0\n",
    "for i in features[:50]:\n",
    "    num+=1\n",
    "    print(\"#{} : {}\".format(num, names[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.19 --- [3 marks] ==========\n",
    "Next, we would like to test the performance of support vector machines. Train three support vector classifiers with the following kernels: linear, radial basis function, and polynomial. Report the classification accuracy of each of the three classifiers on both the training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_linear = SVC(kernel='linear')\n",
    "svc_linear.fit(X_tr2, y_tr)\n",
    "\n",
    "svc_rbf = SVC(kernel='rbf')\n",
    "svc_rbf.fit(X_tr2, y_tr)\n",
    "\n",
    "svc_poly = SVC(kernel='poly')\n",
    "svc_poly.fit(X_tr2, y_tr)\n",
    "\n",
    "print('Linear SVC\\nTraining set: {:.3f}\\nValidation set: {:.3f}'.format(svc_linear.score(X_tr2, y_tr),svc_linear.score(X_val2, y_val)))\n",
    "print('RBF SVC\\nTraining set: {:.3f}\\nValidation set: {:.3f}'.format(svc_rbf.score(X_tr2, y_tr),svc_rbf.score(X_val2, y_val)))\n",
    "print('Polynomial SVC\\nTraining set: {:.3f}\\nValidation set: {:.3f}'.format(svc_poly.score(X_tr2, y_tr),svc_poly.score(X_val2, y_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.20 --- [3 marks] ==========\n",
    "At this point, we would like to get an idea of what kind of decision boundaries we can get with the three types of SVM kernels we introduced in the previous question. For visualisation, however, we can only make use of 2D input data. For this purpose, we select to use the 21st and 76th columns of our training features (*hint: remember that Python uses 0-based indexing*). \n",
    "\n",
    "Execute the cell below to define a useful function which we will be using to plot the decision boundaries *(it is also not a bad idea to try to understand what this functions does)*. \n",
    "\n",
    "Then train three distinct SVM classifiers by using the 2D input data mentioned above and default parameters:\n",
    "* a linear SVC\n",
    "* an RBF SVC \n",
    "* a polynomial SVC\n",
    "\n",
    "Finally, create a list containing the three classifiers you have just trained. Use this list as an input to the provided function along with the used training features and observe the outcome. You can use the additional `title` parameter to set the titles in the subplots. Comment on the shape of the boundaries and what this means for classification accuracy in 1-2 sentences.\n",
    "\n",
    "*(Acknowledgement: this Question has been heavily based on [this example](http://scikit-learn.org/0.17/auto_examples/svm/plot_iris.html) from scikit-learn's documentation.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_svc_decision_boundaries(clfs, X, title=None):\n",
    "    \"\"\"Plots decision boundaries for classifiers with 2D inputs.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    clf : list\n",
    "        Classifiers for which decision boundaries will be displayed.\n",
    "    X : array\n",
    "        Input features used to train the classifiers.\n",
    "    title : list, optional\n",
    "        Titles for classifiers.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    assert X.shape[1] == 2 # Input should be 2D\n",
    "    if title is not None:\n",
    "        assert len(clfs) == len(title)\n",
    "    \n",
    "    h = .04 # step size in the mesh\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    plt.figure(figsize=(15,5))\n",
    "    for i, clf in enumerate(clfs):\n",
    "        plt.subplot(1, len(clfs), i + 1)\n",
    "        plt.subplots_adjust(wspace=0.4, hspace=0.4)\n",
    "        Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.8)\n",
    "        # Training points\n",
    "        plt.scatter(X[:, 0], X[:, 1], c=y_tr, cmap=plt.cm.Paired)\n",
    "        plt.xlabel('Feature 1')\n",
    "        plt.ylabel('Feature 2')\n",
    "        plt.xlim(xx.min(), xx.max())\n",
    "        plt.ylim(yy.min(), yy.max())\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "        if title is not None:\n",
    "            plt.title(title[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_tr2[:,[20,75]]\n",
    "\n",
    "svc_linear = SVC(kernel='linear')\n",
    "svc_linear.fit(X, y_tr)\n",
    "\n",
    "svc_rbf = SVC(kernel='rbf')\n",
    "svc_rbf.fit(X, y_tr)\n",
    "\n",
    "svc_poly = SVC(kernel='poly')\n",
    "svc_poly.fit(X, y_tr)\n",
    "\n",
    "show_svc_decision_boundaries([svc_linear, svc_rbf, svc_poly], X, [\"Linear\", \"RBF\", \"Polynomial\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The boundries are what their kernels' names suggest, the linear is a line the radial bulges towards the axes and the polynomial has a polynomial boundry, but none of them do a very good job of accurately separating the different classes, which would suggest 2D input is not enough to determine which one would be best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.21 --- [5 marks] ==========\n",
    "So far we have used default parameters for training the SVM classifiers. Now we want to tune the parameters by using cross-validation. \n",
    "\n",
    "By using the `K-fold` iterator from Question 1.11 and training data only, estimate the classification accuracy of an SVM classifier with RBF kernel, while you vary the penalty parameter `C` in a logarithmic range `np.logspace(-2, 3, 10)`. Set the kernel coefficient parameter `gamma` to `auto` for this question. \n",
    "\n",
    "Plot the mean cross-validated classification accuracy against the regularisation parameter `C` by using a log-scale for the x-axis. Display the highest obtained mean accuracy score and the value of `C` which yielded it. Label axes appropriately. \n",
    "\n",
    "Finally, train a classifier by using the optimal value for this parameter (without using cross-validation at this stage) and report the classification accuracy on the training and validation sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=3, shuffle=True, random_state=0)\n",
    "kf = kf.split(X_tr2)\n",
    "\n",
    "logspace = np.logspace(start=-2, stop=3, num=10)\n",
    "\n",
    "i = 0\n",
    "arr = np.ndarray((3,10))\n",
    "for kf_train, kf_test in kf:\n",
    "    j = 0\n",
    "    for c in logspace:\n",
    "        svc_rbf = SVC(kernel='rbf', C=c, gamma=\"auto\")\n",
    "        svc_rbf.fit(X_tr2[kf_train], y_tr[kf_train]) \n",
    "        arr[i][j] = (svc_rbf.score(X_tr2[kf_test], y_tr[kf_test]))\n",
    "        j +=1\n",
    "    i +=1\n",
    "\n",
    "mean_arr = arr.mean(axis=0)\n",
    "plt.plot(logspace, mean_arr)\n",
    "plt.semilogx()\n",
    "plt.xlabel(\"C\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Classification Preformance\")\n",
    "plt.show()\n",
    "\n",
    "optimal_c = logspace[mean_arr.argmax()]\n",
    "svc_rbf = SVC(kernel='rbf', C=optimal_c, gamma=\"auto\")\n",
    "svc_rbf.fit(X_tr2, y_tr)\n",
    "print(\"Optimal C: {:.3f}\\nOptimal value: {:.3f}\".format(optimal_c, arr.max()))\n",
    "print('Classification accuracy on training set: {:.3f}'.format(svc_rbf.score(X_tr2, y_tr)))\n",
    "print('Classification accuracy on validation set: {:.3f}'.format(svc_rbf.score(X_val2, y_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.22 --- [5 marks] ==========\n",
    "Now we turn to the kernel coefficient `gamma` parameter. By using the same procedure as in the previous question, estimate the classification accuracy of an SVM classifier with RBF kernel while you vary the `gamma` parameter in a logarithmic range `logspace(-5, 0, 10)`. Fix the penalty parameter `C=1.0`.\n",
    "\n",
    "Plot the mean cross-validated classification accuracy against the parameter `gamma` by using a log-scale for the x-axis. Display the highest obtained mean accuracy score and the value of `gamma` which yielded it.  Label axes appropriately.\n",
    "\n",
    "Finally, train a classifier by using the optimal value for this parameter (without using cross-validation at this stage) and report the classification accuracy on the training and validation sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=3, shuffle=True, random_state=0)\n",
    "kf = kf.split(X_tr2)\n",
    "\n",
    "logspace = np.logspace(start=-5, stop=0, num=10)\n",
    "\n",
    "i = 0\n",
    "arr = np.ndarray((3,10))\n",
    "for kf_train, kf_test in kf:\n",
    "    j = 0\n",
    "    for g in logspace:\n",
    "        svc_rbf = SVC(kernel='rbf', C=1.0, gamma=g)\n",
    "        svc_rbf.fit(X_tr2[kf_train], y_tr[kf_train]) \n",
    "        arr[i][j] = (svc_rbf.score(X_tr2[kf_test], y_tr[kf_test]))\n",
    "        j +=1\n",
    "    i +=1\n",
    "\n",
    "mean_arr = arr.mean(axis=0)\n",
    "plt.plot(logspace, mean_arr)\n",
    "plt.semilogx()\n",
    "plt.xlabel(\"Gamma\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Classification Preformance\")\n",
    "plt.show()\n",
    "\n",
    "optimal_g = logspace[mean_arr.argmax()]\n",
    "svc_rbf = SVC(kernel='rbf', C=1.0, gamma=optimal_g)\n",
    "svc_rbf.fit(X_tr2, y_tr)\n",
    "print(\"Optimal Gamma: {:.3f}\\nOptimal value: {:.3f}\".format(optimal_g, arr.max()))\n",
    "print('Classification accuracy on training set: {:.3f}'.format(svc_rbf.score(X_tr2, y_tr)))\n",
    "print('Classification accuracy on validation set: {:.3f}'.format(svc_rbf.score(X_val2, y_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.23 --- [7 marks] ==========\n",
    "Now we wish to tune both the `C` and `gamma` parameters simultaneously. To save computational time, we will now constrain the parameter search space. Define a `4 X 4` grid for the two parameters, as follows:\n",
    "* `C`: `np.logspace(-2, 1, 4)`\n",
    "* `gamma`: `np.logspace(-4, -1, 4)`\n",
    "\n",
    "Estimate the mean cross-validated classification accuracy by using training data only and all possible configurations for the two parameters. \n",
    "\n",
    "Use a [heatmap](https://seaborn.github.io/generated/seaborn.heatmap.html?highlight=heatmap#seaborn.heatmap) to visualise the mean cross-validated classification accuracy for all `C`-`gamma` pairs. Label axes appropriately and display the values for `C` and `gamma` for the best performing configuration. \n",
    "\n",
    "Finally, by using the optimal configuration, train a classifier (without using cross-validation) and report the classification accuracy on the training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logspace = np.logspace(start=-2, stop=1, num=4)\n",
    "gamma = np.logspace(start= -4, stop=-1, num=4)\n",
    "\n",
    "i = 0\n",
    "arr = np.ndarray((4,4))\n",
    "for c in logspace:\n",
    "    j = 0\n",
    "    for g in gamma:\n",
    "        score = []\n",
    "        kf = KFold(n_splits=3, shuffle=True, random_state=0)\n",
    "        kf = kf.split(X_tr2)\n",
    "        for kf_train, kf_test in kf:\n",
    "            svc_rbf = SVC(kernel='rbf', C=c, gamma=g)\n",
    "            svc_rbf.fit(X_tr2[kf_train], y_tr[kf_train]) \n",
    "            score.append(svc_rbf.score(X_tr2[kf_test], y_tr[kf_test]))\n",
    "        arr[i][j] = sum(score) / len(score)\n",
    "        j +=1\n",
    "    i +=1\n",
    "\n",
    "sns.heatmap(arr, xticklabels=logspace, yticklabels=gamma, vmin=0., vmax=1., annot=True)\n",
    "plt.xlabel(\"Gamma\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Classification Preformance\")\n",
    "plt.show()\n",
    "\n",
    "optimal_c = logspace[arr.argmax(axis=1).max()]\n",
    "optimal_g = gamma[arr.argmax(axis=0).max()]\n",
    "svc_rbf = SVC(kernel='rbf', C=optimal_c, gamma=optimal_g)\n",
    "svc_rbf.fit(X_tr2, y_tr)\n",
    "print(\"Optimal C: {:.3f}\\nOptimal Gamma: {:.3f}\\nOptimal value: {:.3f}\".format(optimal_c, optimal_g, arr.max()))\n",
    "print('Classification accuracy on training set: {:.3f}'.format(svc_rbf.score(X_tr2, y_tr)))\n",
    "print('Classification accuracy on validation set: {:.3f}'.format(svc_rbf.score(X_val2, y_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.24 --- [3 marks] ==========\n",
    "Is the classification accuracy on the validation set higher than in previous questions (1.22-1.23)? If not, can you explain why? Can you think of a way of further improving the performance of the classifier? You don't need to implement your suggestion at this stage. Would there be any associated problems with your suggested approach? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy actually went down. We could imporve the classifier by increasing the number of kFolds, but that would increase computational time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.25 --- [5 marks] ==========\n",
    "Now we want to evaluate the performance of an SVM classifier with polynomial kernel. Once again, we will tune the `degree` parameter by using cross-validation (similarly to what we did in Questions 1.21 and 1.22).\n",
    "\n",
    "By using the `K-fold` iterator from Question 1.11 and training data only, estimate the classification accuracy of polynomial SVM classifier, while you vary the `degree` parameter in the range `np.arange(1,8)`. \n",
    "\n",
    "Plot the mean cross-validated classification accuracy against the polynomial degree. Display the highest obtained mean accuracy score and the value of the `degree` parameter which yielded it. Label axes appropriately. \n",
    "\n",
    "Finally, train a classifier by using the optimal value for this parameter (without using cross-validation at this stage) and report the classification accuracy on the training and validation sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=3, shuffle=True, random_state=0)\n",
    "kf = kf.split(X_tr2)\n",
    "\n",
    "degrees = np.arange(1,8)\n",
    "\n",
    "i = 0\n",
    "arr = np.ndarray((3,7))\n",
    "for kf_train, kf_test in kf:\n",
    "    j = 0\n",
    "    for d in degrees:\n",
    "        svc_poly = SVC(kernel='poly', degree=d)\n",
    "        svc_poly.fit(X_tr2[kf_train], y_tr[kf_train]) \n",
    "        arr[i][j] = (svc_poly.score(X_tr2[kf_test], y_tr[kf_test]))\n",
    "        j +=1\n",
    "    i +=1\n",
    "\n",
    "mean_arr = arr.mean(axis=0)\n",
    "plt.plot(degrees, mean_arr)\n",
    "plt.semilogx()\n",
    "plt.xlabel(\"Degree\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Classification Preformance\")\n",
    "plt.show()\n",
    "\n",
    "optimal_d = degrees[mean_arr.argmax()]\n",
    "svc_poly = SVC(kernel='poly', degree=optimal_d)\n",
    "svc_poly.fit(X_tr2, y_tr)\n",
    "print(\"Optimal Degree: {:.3f}\\nOptimal value: {:.3f}\".format(optimal_d, arr.max()))\n",
    "print('Classification accuracy on training set: {:.3f}'.format(svc_poly.score(X_tr2, y_tr)))\n",
    "print('Classification accuracy on validation set: {:.3f}'.format(svc_poly.score(X_val2, y_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.26 --- [4 marks] ==========\n",
    "\n",
    "You might have noticted that so far, we have used cross-validation for optimising the various tuning parameters (e.g. regularisation parameter in logistic regression, SVM kernel parameters) rather than hold-out validation, although we did have access to a validation set. Why do you think this is a good/bad idea? Give one advantage and one disadvantage of the two different approaches. Which one would you trust more in practice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation doesn't do only one split like the hold-out validation, but trains on all of the training data, so it is more robust against bad splits that don't represent the whole data well. On the other hand it does require more computational resources. In practice i would trust the Cross-validation more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ========== Question 1.27 --- [6 marks] ==========\n",
    "\n",
    "Reload the full training and validation data that contain all indicator variables for all object categories. Remove the `imgId` attribute but keep all of the class indicator variables in the dataset this time. Your training features should include all attributes except `is_person` which should be your target variable. \n",
    "\n",
    "Once again, use a [StandardScaler](http://scikit-learn.org/0.17/modules/generated/sklearn.preprocessing.StandardScaler.html) to standardise your training and validation features. Then train a Random Forest Classifier by using the entropy `criterion`, 500 `n_estimators`, and also set the `random_state` to 31. Report the classification accuracy on the training and validation sets.\n",
    "\n",
    "Similarly to what we did in Question 1.18, order the features by decreasing importance and display the 50 most important features. \n",
    "\n",
    "Finally, answer the following questions:\n",
    "* What do you notice by looking at the list of the best 50 features?\n",
    "* How does the performance differ with respect to the case when the additional class indicator variables are not present (Question 1.16)? Relate your observations to the observed feature ranking.\n",
    "* Would it be easy to make use of the results in practice? Briey explain your reasoning.\n",
    "\n",
    "*(Hint: you might want to look at some of the [images](http://www.inf.ed.ac.uk/teaching/courses/iaml/2014/assts/asst3/images.html) to justify your explanations.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr = train_A.drop([\"imgId\", \"is_person\"], axis=1)\n",
    "y_tr = train_A[\"is_person\"]\n",
    "X_val = valid_A.drop([\"imgId\", \"is_person\"], axis=1)\n",
    "y_val = valid_A[\"is_person\"]\n",
    "\n",
    "scaler = StandardScaler().fit(X_tr)\n",
    "X_tr2 = scaler.transform(X_tr)\n",
    "X_val2 = scaler.transform(X_val)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=500, random_state=31, criterion=\"entropy\")\n",
    "rf.fit(X_tr2, y_tr)\n",
    "print('Classification accuracy on training set: {}'.format(accuracy_score(y_tr, rf.predict(X_tr2))))\n",
    "print('Classification accuracy on validation set: {}'.format(accuracy_score(y_val, rf.predict(X_val2))))\n",
    "\n",
    "features = np.argsort(rf.feature_importances_)[::-1]\n",
    "num = 0\n",
    "for i in features[:50]:\n",
    "    num+=1\n",
    "    print(\"#{} : {}\".format(num, X_tr.columns[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. is_cat and is_bird are at the top so i'm guessing pictures with cats or birds rarely have people on them as wellin the given data set.\n",
    "2. The preformance is a bit better since we have more features to train on, but otherwise the order of importance hasn't changed much\n",
    "3. In practice there's not much correlation between the presance of a cat and a person, for example, in the same photo so deciding on the exsitance of one based on the existance of the other would be a bit pointless."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini challenge [30%]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Important: You are allowed to write up to a maximum of 600 words in this part of the assignment. The thoroughness of the exploration and the quality of the resulting discussion is just as important as the final classification performance of your chosen method(s) and credit will be divided accordingly.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this final part of the assignment we will have a mini object-recognition challenge. Using the data provided you are asked to find the best classiffier for the person/no person classification task. You can apply any preprocessing steps to the data that you think fit and employ any classifier you like (with the provison that you can explain what the classifier is/preprocessing steps are doing). You can also employ any lessons learnt during the course, either from previous Assignments, the Labs or the lecture material to try and squeeze out as much performance as you possibly can. The only restriction is that all steps must be performed in `Python` by using the `numpy`, `pandas` and `sklearn` packages. You can also make use of `matplotlib` and `seaborn` for visualisation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important:** The classification performance metric that we will use for this part is the *cross-entropy* or *logarithmic loss* (see the labs). You should familiarise yourself with the metric by reading the `sklearn` [user guide](http://scikit-learn.org/stable/modules/model_evaluation.html#log-loss) and [documentation](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html#sklearn.metrics.log_loss). To estimate this metric you will need to evaluate probability outputs, as opposed to discrete predictions which we have used so far to compute classification accuracies. Most models in `sklearn` implement a `predict_proba()` method which returns the probabilities for each class. For instance, if your test set consists of `N` datapoints and there are `K` classes, the method will return a `N` x `K` matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide you with three new data sets: a training set (`train_images_partB.csv`), a validation set (`valid_images_partB.csv`), and a test set (`test_images_partB.csv`). You must use the former two for training and evaluating your models (as you see fit). Once you have chosen your favourite model (and pre-processing steps) you should apply it to the test set (for which no labels are provided). Estimate the posterior proabilities for the data points in the test set and submit your results as part of your answer. Your results will be evaluated in terms of the logarithmic loss metric. You also need to submit a brief description of the approaches you considered, your suggested final approach, and a short explanation of why you chose it. The thoroughness of the exploration and the quality of the resulting discussion is just as important as the final score of your chosen method(s) and credit will be divided accordingly.\n",
    "\n",
    "*Hint: Feature engineering, feature combination, model combination and model parameter optimization can significantly improve performance.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to submit your results**: Store the estimated probabilities for the data points in the test set in a 2D numpy array. Then execute the provided cell at the end of this notebook which uses a provided `save_predictions` function to export your results into a `.txt` file (the function will return an error if the provided array has not the right shape). The `.txt` file will be saved where your notebook lives. Submit this file along with your notebook as detailed at the top of this file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer goes here (max. 600 words)*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell to export your results\n",
    "from numpy import savetxt\n",
    "def save_predictions(pred_proba):\n",
    "    if pred_proba.shape != (1114,2):\n",
    "        raise ValueError('Predicted probabilities array is not the right shape.')\n",
    "    \n",
    "    savetxt('assignment_3_predictions.txt', pred_proba)\n",
    "\n",
    "# You need to replace \"test_images_partB_pred_proba\"\n",
    "# with the name of the array which contains the probability \n",
    "# estimates for the data in the test set.\n",
    "save_predictions(test_images_partB_pred_proba) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More information about visual words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual words are based on [Scale-invariant feature transforms (SIFT)](https://en.wikipedia.org/wiki/Scale-invariant_feature_transform). SIFT features are essentially local orientation histograms and capture the properties of small image regions. They possess attractive invariance properties which make them well suited for our task (you can read more about SIFT features in [D.Lowe, IJCV 60(2):91- 110, 2004](http://link.springer.com/article/10.1023/B:VISI.0000029664.99615.94), but the details don't matter for the purpose of this assignment). Each SIFT feature is a 128 dimensional vector. From each image many SIFT features are extracted, typically > 2500 per image (features are extracted at regular intervals using a 15 pixel grid and at 4 different scales). To obtain visual words a representative subset of all extracted SIFT features from all images is chosen and clustered with k-means using 500 centres (such use of the k-means algorithm will be discussed in detail during the lecture). These 500 cluster centres form our visual words. The representation of a single image is obtained by first assigning each SIFT feature extracted from the image to the appropriate cluster (i.e. we determine the visual word corresponding to each feature by picking the closest cluster centre). We then count the number of features from that image assigned to each cluster (i.e. we determine how often each visual word is present in the image). This results in a 500 dimensional count vector for each image (one dimension for each visual word). The normalized version of this count vector gives the final representation of the image (normalized means that we divide the count vector by the total number of visual words in the image, i.e. the normalized counts sum to 1 for each image)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
